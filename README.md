# Generative Models: RealNVP & DDPM

Implementation of two generative models for density estimation and sample generation:
- **RealNVP** - Normalizing Flow using affine coupling layers
- **DDPM** - Denoising Diffusion Probabilistic Model

## Project Structure

```
generative-models/
├── train_realnvp.py      # Train RealNVP model
├── train_ddpm.py         # Train DDPM model
├── 689_data.csv          # 2D dataset (65,536 samples)
├── configs/
│   ├── realnvp.yaml      # RealNVP hyperparameters
│   └── ddpm.yaml         # DDPM hyperparameters
├── models/
│   ├── mlp.py            # Multi-layer perceptron
│   ├── realnvp.py        # RealNVP implementation
│   └── ddpm.py           # DDPM implementation
├── utils/
│   ├── data.py           # Data loading
│   └── visualization.py  # Plotting functions
└── outputs/
    ├── figures/          # Generated plots
    └── checkpoints/      # Saved model weights
```

## Installation

```bash
cd generative-models

# Create virtual environment (optional)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

## Usage

### Train RealNVP
```bash
python train_realnvp.py --config configs/realnvp.yaml

# With custom settings
python train_realnvp.py --epochs 200 --batch_size 1024
```

### Train DDPM
```bash
python train_ddpm.py --config configs/ddpm.yaml

# With custom settings
python train_ddpm.py --epochs 500 --batch_size 2048
```

### Command-line Options

| Option | Default | Description |
|--------|---------|-------------|
| `--config` | `configs/*.yaml` | Path to config file |
| `--epochs` | 500/1000 | Number of training epochs |
| `--batch_size` | 2048/4096 | Training batch size |
| `--lr` | 0.001 | Learning rate |
| `--seed` | 42/689 | Random seed |
| `--data` | `689_data.csv` | Path to dataset |

---

## Output Images

After training, three-panel visualizations are saved to `outputs/figures/`:

### RealNVP Results (`realnvp_results.png`)

| Panel | Description |
|-------|-------------|
| **Original Data** | The input 2D distribution (e.g., two moons, spiral) |
| **Latent z = f(x)** | Data transformed to latent space - should look like a standard Gaussian N(0,I) |
| **Generated Samples** | New samples generated by sampling z ~ N(0,I) and applying inverse transform f⁻¹(z) |

### DDPM Results (`ddpm_results.png`)

| Panel | Description |
|-------|-------------|
| **Original Data** | The input 2D distribution |
| **Forward Diffused** | Data after full forward diffusion (should look like Gaussian noise) |
| **Generated Samples** | Samples generated through learned reverse diffusion process |

### Loss Curves

- `realnvp_loss.png` - Negative log-likelihood over training epochs
- `ddpm_loss.png` - Weighted MSE loss over training epochs

---

## Model Explanations

### RealNVP (Real-valued Non-Volume Preserving)

RealNVP is a **normalizing flow** model that learns an invertible transformation between complex data distributions and a simple base distribution (standard Gaussian).

**How it works:**

1. **Affine Coupling Layers**: The input is split in half. One half passes through unchanged while parameterizing an affine transformation (scale and shift) applied to the other half.

2. **Invertibility**: Because only half the input is transformed at a time, the inverse is trivially computable - just reverse the affine transformation.

3. **Stacking Layers**: Multiple coupling layers with alternating splits ensure all dimensions interact.

4. **Training**: Maximizes log-likelihood using the change-of-variables formula:
   ```
   log p(x) = log p(z) + log |det(∂f/∂x)|
   ```

**Key Properties:**
- Exact likelihood computation
- Efficient sampling via inverse transform
- Latent space is interpretable (Gaussian)

### DDPM (Denoising Diffusion Probabilistic Model)

DDPM is a **diffusion-based generative model** that learns to reverse a gradual noising process.

**How it works:**

1. **Forward Process**: Construct a Markov chain that gradually adds noise over L steps until data becomes pure Gaussian noise:
   ```
   q(x_{i+1} | x_i) = N(x_{i+1} | √(1-β²_{i+1}) · x_i, β²_{i+1} · I)
   ```
   
   The noise schedule uses a linear interpolation:
   ```
   β²_i = β²_0 + (i/L)(β²_1 - β²_0)
   ```
   where β²_0 = 10⁻⁴ and β²_1 = 0.02 by default.

2. **Closed-form Sampling**: We can sample x_i directly from x_0 using:
   ```
   q(x_i | x_0) = N(x_i | ᾱ_i · x_0, (1 - ᾱ²_i) · I)
   ```
   where ᾱ_i = ∏_{k=1}^{i} α_k and α_i = √(1 - β²_i)

3. **Reverse Process**: Learn a neural network μ_θ(x_t, t) to predict the posterior mean for denoising:
   ```
   p_θ(x_{i-1} | x_i) = N(x_{i-1} | μ_θ(x_i, i), σ²_i · I)
   ```

4. **Training**: The network learns to predict the true posterior mean μ̃ given the noisy input and timestep, minimizing a weighted MSE loss.

**Key Properties:**
- As i → ∞, ᾱ_i → 0, so q(x_L | x_0) ≈ N(0, I)
- State-of-the-art sample quality
- Stable training (no mode collapse)
- Computationally expensive sampling (many denoising steps)

---

## Configuration Files

### `configs/realnvp.yaml`
```yaml
model:
  n_coupling_layers: 14    # Number of coupling layers
  n_hidden: 2              # Hidden layers per coupling network
  hidden_units: 128        # Units per hidden layer

training:
  epochs: 500
  batch_size: 2048
  learning_rate: 0.001
  lr_decay: 0.999          # Exponential LR decay
```

### `configs/ddpm.yaml`
```yaml
model:
  diffusion_steps: 2500    # Number of diffusion timesteps
  beta2_start: 0.0001      # Start of β² schedule
  beta2_end: 0.02          # End of β² schedule
  time_embed_dim: 32       # Time embedding dimension
  hidden_units: 64         # Hidden units in denoising network

training:
  epochs: 1000
  batch_size: 4096
  learning_rate: 0.001
  lr_decay: 0.999
```

---

## References

- **RealNVP**: Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). *Density estimation using Real-NVP*. ICLR.
- **DDPM**: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising Diffusion Probabilistic Models*. NeurIPS.
